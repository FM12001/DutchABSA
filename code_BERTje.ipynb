{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTje.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "XtfqiQsdKp_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Die8e_RyKlo0"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"GroNLP/bert-base-dutch-cased\", num_labels = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/drive/MyDrive/ResearchProject/083/reviews_083_training.json') as f: #specify training data path\n",
        "    data = json.load(f) #[{\"text\": \"blabla\", \"pros\": [\"bla\", \"bla\"], \"cons\": [\"bla\"]}, {...}]\n",
        "\n",
        "new_data = []\n",
        "new_labels = []\n",
        "for review in data:\n",
        "        for pro in review[\"pros\"]:\n",
        "          temp = (review[\"text\"], pro)\n",
        "          new_data.append(temp)\n",
        "          new_labels.append(1)\n",
        "        for con in review[\"cons\"]:\n",
        "          temp = ()\n",
        "          temp = (review[\"text\"], con)\n",
        "          new_data.append(temp)\n",
        "          new_labels.append(0)\n",
        "print(new_data[:10])\n",
        "print(new_labels[:10])"
      ],
      "metadata": {
        "id": "GPUzwUNagxOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adapted from https://huggingface.co/docs/transformers/v4.19.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus\n",
        "#and https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
        "#...and a few more resources"
      ],
      "metadata": {
        "id": "C3LhQSKKvVML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split into train and test, but first generate a seed (at random) and memorize it,\n",
        "#so we can apply the exact same split to the labels\n",
        "import random\n",
        "less_random = n = random.randint(0,1000)\n",
        "\n",
        "\n",
        "train, test = train_test_split(new_data,test_size=0.15,random_state=less_random) \n",
        "train_labels, test_labels = train_test_split(new_labels,test_size=0.15,random_state=less_random) \n",
        "\n",
        "\n",
        "train_batch = tokenizer.batch_encode_plus(\n",
        "      batch_text_or_text_pairs = train, #the sentence + the aspect\n",
        "      add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "      padding=\"max_length\",           # Max length to truncate/pad, 512 is Max for BERT and we have longer reviews than that\n",
        "      truncation=\"only_first\",        # if the review is too long, truncate the review, _not_ the aspect\n",
        "      return_attention_mask = True\n",
        ")\n",
        "\n",
        "\n",
        "test_batch = tokenizer.batch_encode_plus(\n",
        "       batch_text_or_text_pairs = test, #the sentence + the aspect\n",
        "       add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "       padding=\"max_length\",           # Max length to truncate/pad, 512 is Max for BERT and we have longer reviews than that\n",
        "       truncation=\"only_first\",        # if the review is too long, truncate the review, _not_ the aspect\n",
        "       return_attention_mask = True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))\n",
        "\n",
        "print(train[0])\n",
        "print(train_batch['input_ids'][0])"
      ],
      "metadata": {
        "id": "SBz_WCiFubG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):    \n",
        "    def __init__(self, encodings, labels=None):          \n",
        "        self.encodings = encodings        \n",
        "        self.labels = labels\n",
        "     \n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ],
      "metadata": {
        "id": "CZwujhNZ0vak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(train_batch, train_labels)\n",
        "test_dataset = Dataset(test_batch, test_labels)"
      ],
      "metadata": {
        "id": "qPiyb0rl_vY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer # source https://huggingface.co/docs/transformers/training\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(p):    \n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    return {\"accuracy\": accuracy} \n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/Bertje\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True,\n",
        "    save_steps=1000,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bEsFRnNyvFTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()\n",
        "#uncomment to download model in one go\n",
        "# !zip -r /content/bertje.zip /content/Bertje\n",
        "# files.download('/content/bertje.zip')"
      ],
      "metadata": {
        "id": "rvmMZoSdZHIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "5WZ7H1Nq3mFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"/content/Bertje\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")"
      ],
      "metadata": {
        "id": "4cZQTl4l3dkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "HRiuM4iQ6IaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run predictions for entire file\n",
        "import json\n",
        "from transformers import Trainer\n",
        "\n",
        "test_trainer = Trainer(model)\n",
        "\n",
        "with open('/content/drive/MyDrive/ResearchProject/083/Bertje/newModel/result_wo_class.json') as file: #specify file with generated aspects\n",
        "  test_data = json.load(file)\n",
        "\n",
        "#edit for specific data format\n",
        "def predict_review(review):\n",
        "  review[\"predictions\"] = {}\n",
        "  for aspect in review['generated_aspects']:\n",
        "    X_test = [(review['text'], aspect['aspect'])]\n",
        "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=\"only_first\", max_length=512)\n",
        "    pred,_,_ = test_trainer.predict(Dataset(X_test_tokenized))\n",
        "    review['predictions'][aspect[\"aspect\"]] = round(pred.flatten().tolist()[0])\n",
        "\n",
        "for review in test_data:\n",
        "  predict_review(review)\n",
        "\n",
        "with open('/content/drive/MyDrive/ResearchProject/083/Bertje/results.json', 'w') as outfile:\n",
        "  json.dump(test_data, outfile)\n"
      ],
      "metadata": {
        "id": "VsElDHBSPOtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run a single prediction\n",
        "test_trainer = Trainer(model)\n",
        "\n",
        "test_text = Dataset(tokenizer([(\"beeld is echt verschrikkelijk goed\", \"beeld\")], padding=True, truncation=\"only_first\", max_length=512))\n",
        "raw_pred_one, _, _ = test_trainer.predict(test_text)\n",
        "raw_pred_one.flatten()"
      ],
      "metadata": {
        "id": "xzmGRwPDJRzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}