{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_without_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpLqnela035i"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/drive/MyDrive/ResearchProject/083/reviews_083_training.json') as f: #path to training data\n",
        "    data = json.load(f)\n",
        "\n",
        "train, test = train_test_split(data,test_size=0.15) \n",
        "\n",
        "def build_text_files_no_class(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for review in data_json:\n",
        "        aspects = \"\"\n",
        "        for pro in review[\"pros\"]:\n",
        "          aspects += f'<aspect>{pro}'\n",
        "        for con in review[\"cons\"]:\n",
        "          aspects += f'<aspect>{con}'\n",
        "        text = f'<startoftext><reviewtext>{review[\"text\"]}<aspects>{aspects}<endoftext>'\n",
        "        data += text + \"\\n\"\n",
        "    f.write(data)\n",
        "\n",
        "build_text_files_no_class(train,'train_dataset.txt')\n",
        "build_text_files_no_class(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))\n"
      ],
      "metadata": {
        "id": "Z2Ppi6tl2rS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-dutch\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ],
      "metadata": {
        "id": "WX0NGlF89TEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "     \n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ],
      "metadata": {
        "id": "eGS_pKh292_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"GroNLP/gpt2-small-dutch\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-without-classification\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=32 # batch size for training\n",
        "    per_device_eval_batch_size=64, # batch size for evaluation\n",
        "    eval_steps = 200, # Number of update steps between two evaluations.\n",
        "    save_steps=2000000, # after # steps model is saved \n",
        "    warmup_steps=250, # number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "ke6_fnL8-BMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "3M1434jh-p8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('text-generation', model='./gpt2-without-classification', tokenizer=\"GroNLP/gpt2-small-dutch\")"
      ],
      "metadata": {
        "id": "qDVsL5DT-yIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"<startoftext><reviewtext>Geweldige wasmachine, draait hard. Maar wel luidruchtig. Ik vind het lawaai erg vervelend. De was wordt wel goed schoon. zeer mooie kleur<aspects>\")"
      ],
      "metadata": {
        "id": "lLPiKU7L_GDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"<startoftext><reviewtext>De stoel zit lekker, maar ziet er niet uit.<aspects>\")"
      ],
      "metadata": {
        "id": "-ue-SP_Scu6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"<startoftext><reviewtext>Mooi design maar de software is slecht<aspects>\")"
      ],
      "metadata": {
        "id": "VHRp8S19KVrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"<startoftext><reviewtext>mooi ding hoor<aspects>\")"
      ],
      "metadata": {
        "id": "BPfG8jDWKV3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to download the model\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/gpt2-without-classification.zip /content/gpt2-without-classification\n",
        "files.download('/content/gpt2-without-classification.zip')"
      ],
      "metadata": {
        "id": "j4sJkJ2S7xK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load existing model"
      ],
      "metadata": {
        "id": "BRgMuuGdh_St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelWithLMHead\n",
        "model = AutoModelWithLMHead.from_pretrained(\"/content/model\")"
      ],
      "metadata": {
        "id": "esmdYxaNi4Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "pipe = pipeline('text-generation', model=\"/content/drive/MyDrive/ResearchProject/080/GPT2_without_class/model\", tokenizer=\"GroNLP/gpt2-small-dutch\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-dutch\")"
      ],
      "metadata": {
        "id": "P7DkQNvXj3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model"
      ],
      "metadata": {
        "id": "k_10zxl2kQwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ILeH3u1ULbPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/ResearchProject/083/reviews_083_testing.json') as f:\n",
        "   test_data = json.load(f)"
      ],
      "metadata": {
        "id": "y8TRTmYkNCvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#specify output path:\n",
        "outpath = '/content/drive/MyDrive/ResearchProject/083/GPT2_without_class/GPT2_without_class_083_results.json'\n",
        "\n",
        "def check_count():\n",
        "  if count % 100 == 0:\n",
        "    print(\"processed \"+str(count)+\" reviews out of \" + str(total_num_reviews))\n",
        "    with open(outpath, 'w') as outfile: \n",
        "      json.dump(test_data, outfile)\n",
        "\n",
        "\n",
        "count = 0\n",
        "total_num_reviews = len(test_data)\n",
        "\n",
        "for review in test_data:\n",
        "  if (\"generated_aspects\" in review.keys()) and review[\"generated_aspects\"]:\n",
        "    count += 1\n",
        "    check_count()\n",
        "    continue\n",
        "  else:\n",
        "    review[\"generated_aspects\"] = []\n",
        "  \n",
        "  max_length = len(tokenizer(review[\"text\"])[\"input_ids\"]) + 15 + 15\n",
        "  generated = pipe(\"<startoftext><reviewtext>\" + review[\"text\"] + \"<aspects>\", max_length = max_length)\n",
        "  generated = generated[0][\"generated_text\"]\n",
        "  generated = generated.replace(f'<startoftext><reviewtext>{review[\"text\"]}<aspects>',\"\")\n",
        "\n",
        "  if (not generated.startswith(\"<endoftext>\")) and (\"<endoftext>\" in generated):\n",
        "    generated = generated.split(\"<endoftext>\")[0]\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "  generated = generated.split(\"<aspect>\")\n",
        "  for substring in generated:\n",
        "    if substring:\n",
        "      review[\"generated_aspects\"].append(substring)\n",
        "  count += 1\n",
        "  check_count()\n",
        "\n",
        "\n",
        "with open(outpath, 'w') as outfile:\n",
        "  json.dump(test_data, outfile)\n",
        "\n",
        "files.download(outpath)"
      ],
      "metadata": {
        "id": "BW0yuWl4kUZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}